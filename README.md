# WebCrawler
create sitemap for website

- Developed a multi-threaded website crawler written in Python to extract all links from given domain.
- Crawler starts with any website with given domain to visit, called the seed. As the crawler visits this URL, 
it identifies all the hyperlinks in the page and adds them to the list of URLs to visit, called the crawl frontier, 
which becomes the seeds of next execution.
-Website crawler can avoid duplicate url and create sitemap for website efficiently.

Thanks to the tutorial of Bucky Roberts!
